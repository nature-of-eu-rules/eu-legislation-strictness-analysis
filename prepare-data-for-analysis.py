#!/usr/bin/env python
# coding: utf-8

"""
Script to enrich the metadata file generated by https://github.com/nature-of-eu-rules/data-extraction/blob/main/eu_rules_metadata_extractor.py
with regulatory statement classification results generated by either:

- https://github.com/nature-of-eu-rules/regulatory-statement-classification/blob/main/classify_text_with_inlegal_bert_xgboost.py (legalbert classifier) or,
- https://github.com/nature-of-eu-rules/regulatory-statement-classification/blob/main/rule-based-classification.py (gramatical parser)

The output of this script is the main input file required by https://github.com/nature-of-eu-rules/eu-legislation-strictness-analysis/blob/main/analysis.py
"""

import pandas as pd
import argparse
import sys
from os.path import exists
import os

argParser = argparse.ArgumentParser(description='Script to enrich EU legislative document metadata with information about the number of regulatory statements contained within')
required = argParser.add_argument_group('required arguments')
required.add_argument("-m", "--metadata", required=True, help="Path to input CSV file generated by http://github.com/nature-of-eu-rules/data-extraction/blob/main/eu_rules_metadata_extractor.py")
required.add_argument("-c", "--classification", required=True, help="Path to input CSV file generated by either https://github.com/nature-of-eu-rules/regulatory-statement-classification/blob/main/classify_text_with_inlegal_bert_xgboost.py or https://github.com/nature-of-eu-rules/regulatory-statement-classification/blob/main/rule-based-classification.py")
required.add_argument("-o", "--output", required=True, help="Path to output CSV file which is the enriched metadata ready to be analysed by https://github.com/nature-of-eu-rules/eu-legislation-strictness-analysis/blob/main/analysis.py")
args = argParser.parse_args()

METADATA_FNAME = str(args.metadata) # Metadata input filename
CLASSIF_FNAME = str(args.classification) # Classification results input filename
OUT_FNAME = str(args.output) # Output filename

# Import data
sent_df = pd.read_csv(CLASSIF_FNAME)
metadata_df = pd.read_csv(METADATA_FNAME)

# Filter out irrelevant columns
sent_df = sent_df[['celex', 'sent', 'regulatory_according_to_rule', 'sent_count', 'word_count']]

# Separate regulatory and constitutive sentences
regulatory_df = sent_df[sent_df['regulatory_according_to_rule'] == 1.0]
constitutive_df = sent_df[sent_df['regulatory_according_to_rule'] == 0.0]

# Count number of regulatory sentences in each document and create a new dataframe with the results
regulation_counts_df = regulatory_df.groupby(['celex'])['sent'].count()
rc_ind = regulation_counts_df.reset_index(drop=False)
rc = pd.DataFrame(rc_ind.values.tolist(), columns=['celex', 'reg_count'])

# Initialise new data lists (columns) to add to the metadata file
col_reg_counts = []
sent_counts = []
word_counts = []

# Get the unique document IDs in both input dataframes 
# (they should match but may not) if they do not match it means that the metadata
# file has more IDs than the classification results, which makes sense because
# not all documents have regulatory or even potentially regulatory sentences.
celexes = list(set(rc['celex'].tolist())) # unique IDs in classification results file
m_celexes = metadata_df['celex'].tolist() # unique IDs in the input metadata file

idx = 0
count = 0

# Update the metadata file with the number of regulatory statements in each document
for m_celex in m_celexes:
    idx += 1
    if idx % 1000 == 0:
        print(idx, '/', len(m_celexes))

    if m_celex in celexes:
        matching_item = rc[rc['celex'] == m_celex]
        col_reg_counts.append(matching_item['reg_count'].tolist()[0])
        mat = sent_df[sent_df['celex'] == m_celex]
        sent_counts.append(mat['sent_count'].tolist()[0])
        word_counts.append(mat['word_count'].tolist()[0])
    else:
        col_reg_counts.append(0)
        mat = sent_df[sent_df['celex'] == m_celex]
        if len(mat) > 0:
            sent_counts.append(mat['sent_count'].tolist()[0])
            word_counts.append(mat['word_count'].tolist()[0])
        else:
            count += 1
            sent_counts.append(1) # no sentences just set to 1 to avoid division by zero in any calculations
            word_counts.append(1) # no words just set to 1 to avoid division by zero in any calculations

metadata_df['reg_count'] = col_reg_counts
metadata_df['sent_count'] = sent_counts
metadata_df['word_count'] = word_counts

# Use a single adoption date value put this date in a new column 
# (in some cases there are multiple adoption dates for the same document)
canon_adoption_dates = []
for index, row in metadata_df.iterrows():
    if len(row['date_adoption'].split('|')) > 1:
        cdate = row['date_adoption'].split('|')[0]
        curr_date = pd.to_datetime(cdate.strip(), format='%Y-%m-%d')
        canon_adoption_dates.append(curr_date)
    else:
        curr_date = pd.to_datetime(row['date_adoption'], format='%Y-%m-%d')
        canon_adoption_dates.append(curr_date)

metadata_df['date'] = canon_adoption_dates

# Save enriched metadata to CSV file
metadata_df.to_csv(os.path(OUT_FNAME), index=False)
